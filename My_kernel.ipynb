{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House price prediction, exercise #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import sklearn\n",
    "import seaborn as sb\n",
    "import math\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* (1) 1460개 행 중에서 1000개 이상의 결측값을 가진 열 삭제 \n",
    "* (2) 수치형 변수와 범주형 변수로 나누어서 결측치 대체\n",
    "* (3) 범주형 데이터는 카디널리티 10 기준으로 나누어서 각각 Oridinal, OneHot으로 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# (1)\\nloss_cols  = [col for col in train if train[col].isnull().sum() > 1000]\\ntrain_f = train.drop(loss_cols, axis=1)\\n\\n\\n# (2)\\ntrain_num_cols = [col for col in train_f if train_f[col].dtypes !=\\'object\\' ]\\nsim = SimpleImputer()\\ntrain_num = pd.DataFrame(sim.fit_transform(train_f[train_num_cols]), columns=train_num_cols)\\n\\ntrain_cat = train_f.select_dtypes(include=\\'object\\')\\ncat_sim = SimpleImputer(strategy=\\'most_frequent\\')\\ntrain_cat = pd.DataFrame(cat_sim.fit_transform(train_cat), columns=train_cat.columns)\\n\\n# (3)\\nhigh_cardinal_cols = [col for col in train_cat.columns if train_cat[col].nunique() >= 10]\\nlow_cardinal_cols = [col for col in train_cat.columns if train_cat[col].nunique() < 10]\\n\\nore = OrdinalEncoder()\\ntrain_ohe = pd.get_dummies(train_cat[low_cardinal_cols],  prefix=low_cardinal_cols, prefix_sep=\\'_\\') #pd.DataFrame(ohe.fit_transform(train_cat[low_cardinal_cols]))\\ntrain_ore = pd.DataFrame(ore.fit_transform(train_cat[high_cardinal_cols]), columns = high_cardinal_cols)\\n\\n# concatenation\\ntrain_f.drop(train_cat.columns, axis=1, inplace=True)\\ntrain_f.drop(train_num.columns, axis=1, inplace=True)\\n\\ntrain_f = pd.concat([train_num, train_ohe, train_ore], axis=1)\\n\\n# # of joined dataframe\\'s col is 223\\nprint(\\'====null values====\\')\\nprint(train_f.isnull().sum().sum())\\nprint(\\'====Validation====\\')\\nprint(len(train_cat.columns), len(train_num.columns))\\nprint(len(train_ohe.columns), len(train_ore.columns), len(train_num.columns))\\nprint(\"Valid : \" ,((len(train_ohe.columns)+len(train_ore.columns)+len(train_num.columns)) == 223))\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# (1)\n",
    "loss_cols  = [col for col in train if train[col].isnull().sum() > 1000]\n",
    "train_f = train.drop(loss_cols, axis=1)\n",
    "\n",
    "\n",
    "# (2)\n",
    "train_num_cols = [col for col in train_f if train_f[col].dtypes !='object' ]\n",
    "sim = SimpleImputer()\n",
    "train_num = pd.DataFrame(sim.fit_transform(train_f[train_num_cols]), columns=train_num_cols)\n",
    "\n",
    "train_cat = train_f.select_dtypes(include='object')\n",
    "cat_sim = SimpleImputer(strategy='most_frequent')\n",
    "train_cat = pd.DataFrame(cat_sim.fit_transform(train_cat), columns=train_cat.columns)\n",
    "\n",
    "# (3)\n",
    "high_cardinal_cols = [col for col in train_cat.columns if train_cat[col].nunique() >= 10]\n",
    "low_cardinal_cols = [col for col in train_cat.columns if train_cat[col].nunique() < 10]\n",
    "\n",
    "ore = OrdinalEncoder()\n",
    "train_ohe = pd.get_dummies(train_cat[low_cardinal_cols],  prefix=low_cardinal_cols, prefix_sep='_') #pd.DataFrame(ohe.fit_transform(train_cat[low_cardinal_cols]))\n",
    "train_ore = pd.DataFrame(ore.fit_transform(train_cat[high_cardinal_cols]), columns = high_cardinal_cols)\n",
    "\n",
    "# concatenation\n",
    "train_f.drop(train_cat.columns, axis=1, inplace=True)\n",
    "train_f.drop(train_num.columns, axis=1, inplace=True)\n",
    "\n",
    "train_f = pd.concat([train_num, train_ohe, train_ore], axis=1)\n",
    "\n",
    "# # of joined dataframe's col is 223\n",
    "print('====null values====')\n",
    "print(train_f.isnull().sum().sum())\n",
    "print('====Validation====')\n",
    "print(len(train_cat.columns), len(train_num.columns))\n",
    "print(len(train_ohe.columns), len(train_ore.columns), len(train_num.columns))\n",
    "print(\"Valid : \" ,((len(train_ohe.columns)+len(train_ore.columns)+len(train_num.columns)) == 223))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing. V2\n",
    "\n",
    "* Log transformation (@ skewed featrues)\n",
    "* Encoding categorial values \n",
    "* Null value imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 288) (1460, 288) (1459, 288)\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\n",
    "skewed_feats  = train[numeric_feats].apply(lambda x : skew(x.dropna()) > 0.75).index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "#2\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#3\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "train_f = all_data[:train.shape[0]]\n",
    "test_f = all_data[train.shape[0]:]\n",
    "train_y = train.SalePrice\n",
    "\n",
    "print(all_data.shape, train_f.shape, test_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "*  #1. train_test_split\n",
    "*  #2. Cross_val_score\n",
    "*  #3. Kfold\n",
    "*  #4. KFold + hyperparameter tuning(GCV)\n",
    "*  #5. Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01319954 0.01929687 0.01713127 0.01394279 0.01934873]\n",
      "root_mean_squared_error:  0.12877825831898793\n"
     ]
    }
   ],
   "source": [
    "flag = 2\n",
    "if flag == 1: # error : \n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_f,train_y, train_size=0.8, test_size=0.2)\n",
    "\n",
    "    model = XGBRegressor(eta=0.1, colsample_bytree=0.75, max_depth= 3, min_child_weight=3, eval_metric=\"rmse\")\n",
    "    model.fit(train_x,train_y)\n",
    "\n",
    "    pred = model.predict(valid_x)\n",
    "\n",
    "    score = mean_squared_error(pred, valid_y, squared=False)\n",
    "    print(score)\n",
    "    print(\"root_mean_squared_error: \", score)\n",
    "    \n",
    "elif flag == 2:  \n",
    "    model = XGBRegressor(eta=0.1, colsample_bytree=0.75, max_depth= 3, min_child_weight=3, eval_metric=\"rmse\")\n",
    "    model.fit(train_f,train_y)\n",
    "    scores = -1*cross_val_score(model, train_f, train_y, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(scores)\n",
    "    print(\"root_mean_squared_error: \",math.sqrt(scores.mean()))\n",
    "elif flag == 3:\n",
    "    pass\n",
    "elif flag == 4:\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_f, train_y, train_size=0.8, test_size = 0.2)\n",
    "    \n",
    "    model = XGBRegressor()\n",
    "    kf = KFold(random_state=30, shuffle=True, n_splits=5)\n",
    "    params = {'eta':[0.05, 0.1],'max_depth':[5,7], 'min_child_weight':[1,3], 'colsample_bytree':[0.5,0.75]}\n",
    "    \n",
    "    gcv = GridSearchCV(estimator=model, cv=kf, n_jobs=10, scoring='neg_mean_squared_error', verbose=True, param_grid=params)\n",
    "    \n",
    "    \n",
    "    gcv.fit(train_x, train_y)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    model = gcv.best_estimator_\n",
    "    pred = model.predict(valid_x)\n",
    "    score = mean_squared_error(pred, valid_y, squared=False)\n",
    "    \n",
    "    print(\"mean_squared_error: \",math.sqrt(score))\n",
    "#     Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
    "#     {'colsample_bytree': 0.75, 'max_depth': 5, 'min_child_weight': 3}\n",
    "#     Mean_absolute_error:  17648.85913420377\n",
    "elif flag == 5 : \n",
    "    def rmse_cv(model):\n",
    "        rmse= np.sqrt(-cross_val_score(model, train_f, train_y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "        return rmse\n",
    "    alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10 ,15, 30, 50, 75]\n",
    "    cv_ridge = [rmse_cv(Ridge(alpha=alpha)).mean() for alpha in alphas]\n",
    "    cv_ridge = pd.Series(cv_ridge, index=alphas)\n",
    "    cv_ridge.plot()\n",
    "\n",
    "    train_f.describe()\n",
    "    score  = cv_ridge.min()\n",
    "    print('score', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res = best_model.predict(test_f)\n",
    "pred_res = np.exp(pred_res)\n",
    "\n",
    "sub_df = pd.DataFrame({\"Id\":test.Id, \"SalePrice\":pred_res})\n",
    "sub_df.to_csv('./data/my_kernel_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
